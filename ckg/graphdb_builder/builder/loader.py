"""
    Populates the graph database with all the files generated by the importer.py module:
    Ontologies, Databases and Experiments.
    The module loads all the entities and relationships defined in the importer files. It
    calls Cypher queries defined in the cypher.py module. Further, it generates an hdf object
    with the number of enities and relationships loaded for each Database, Ontology and Experiment.
    This module also generates a compressed backup file of all the loaded files.

    There are two types of updates:

    - Full: all the entities and relationships in the graph database are populated
    - Partial: only the specified entities and relationships are loaded

    The compressed files for each type of update are named accordingly and saved in the archive/ folder
    in data/.
"""

import os
from urllib.parse import quote, unquote
import sys
import re
from datetime import datetime
from ckg import ckg_utils
from ckg.graphdb_connector import connector
from ckg.graphdb_builder import builder_utils


START_TIME = datetime.now()

try:
    ckg_config = ckg_utils.read_ckg_config()
    cwd = os.path.dirname(os.path.abspath(__file__))
    log_config = ckg_config['graphdb_builder_log']
    logger = builder_utils.setup_logging(log_config, key="loader")
    config = builder_utils.setup_config('builder')
except Exception as err:
    logger.error("Reading configuration > {}.".format(err))

def load_into_database(driver, queries, requester):
    """
    This function runs the queries provided in the graph database using a neo4j driver.

    :param driver: neo4j driver, which provides the connection to the neo4j graph database.
    :type driver: neo4j driver
    :param list[dict] queries: list of queries to be passed to the database.
    :param str requester: identifier of the query.
    """
    regex = r"file:\/\/\/(.+\.tsv)"
    result = None
    for query in queries:
        try:
            if query.strip().lower().startswith("call apoc.periodic.iterate"):
                # This is an APOC query, don't add a semicolon
                result = connector.commitQuery(driver, query)
            elif "file" in query:
                matches = re.search(regex, query)
                if matches:
                    file_path = matches.group(1)
                    if os.path.isfile(unquote(file_path)):
                        # Add semicolon only if it's not already there
                        result = connector.commitQuery(driver, query if query.strip().endswith(';') else query + ";")
                        record = result.single()
                        if record is not None and 'c' in record:
                            counts = record['c']
                            if counts == 0:
                                logger.warning("{} - No data was inserted in query: {}.\n results: {}".format(requester, query, counts))
                            else:
                                logger.info("{} - Query: {}.\n results: {}".format(requester, query, counts))
                        else:
                            logger.info("{} - cypher query: {}".format(requester, query))
                    else:
                        logger.error("Error loading: File does not exist. Query: {}".format(query))
            else:
                # Add semicolon only if it's not already there
                result = connector.commitQuery(driver, query if query.strip().endswith(';') else query + ";")
        except Exception as err:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            logger.error("Loading: {}, file: {}, line: {} - query: {}".format(err, fname, exc_tb.tb_lineno, query))

    return result

def updateDB(driver, imports=None, specific=[]):
    if imports is None:
        imports = config["graph"]
    try:
        cypher_queries = ckg_utils.get_queries(os.path.join(cwd, config['cypher_queries_file']))
    except Exception as err:
        logger.error("Reading queries file > {}.".format(err))

    for i in imports:
        queries = []
        logger.info("Loading {} into the database".format(i))
        try:
            import_dir = quote(ckg_config['imports_databases_directory'], safe='/:')
            
            if i == "ontologies":
                entities = [e.lower() for e in config["ontology_entities"]]
                if len(specific) > 0:
                    entities = list(set(entities).intersection([s.lower() for s in specific]))
                import_dir = quote(ckg_config['imports_ontologies_directory'], safe='/:')
                ontologyDataImportCode = cypher_queries['IMPORT_ONTOLOGY_DATA']['query']
                for entity in entities:
                    queries.append(ontologyDataImportCode.replace("ENTITY", entity.capitalize()).replace("IMPORTDIR", import_dir))
                mappings = config['ontology_mappings']
                mappingImportCode = cypher_queries['IMPORT_ONTOLOGY_MAPPING_DATA']['query']
                for m in mappings:
                    if m.lower() in entities:
                        for r in mappings[m]:
                            queries.append(mappingImportCode.replace("ENTITY1", m).replace("ENTITY2", r).replace("IMPORTDIR", import_dir))
            
            elif i in ["biomarkers", "qcmarkers"]:
                code = cypher_queries[f'IMPORT_{i.upper()}']['query']
                import_dir = quote(ckg_config['imports_curated_directory'], safe='/:')
                queries.append(code.replace("IMPORTDIR", import_dir))
            
            # For other imports that use APOC, follow the same pattern:
            elif i in ["chromosomes", "genes", "transcripts", "proteins", "functional_regions", "annotations", 
                       "protein_structure", "gwas", "known_variants"]:
                code = cypher_queries[f'IMPORT_{i.upper()}_DATA']['query']
                queries.append(code.replace("IMPORTDIR", import_dir))
            
            # For imports that iterate over resources:
            elif i in ["complexes", "modified_proteins", "pathology_expression", "ppi", "diseases", "drugs", 
                       "side_effects", "pathway", "metabolite", "food", "clinical_variants"]:
                for query_key in cypher_queries.keys():
                    if query_key.startswith(f'IMPORT_{i.upper()}'):
                        code = cypher_queries[query_key]['query']
                        resource_list = config.get(f"{i}_resources", [])
                        if isinstance(resource_list[0], tuple):  # For cases like diseases
                            for entity, resource in resource_list:
                                queries.append(code.replace("IMPORTDIR", import_dir)
                                                   .replace("ENTITY", entity)
                                                   .replace("RESOURCE", resource.lower()))
                        else:
                            for resource in resource_list:
                                queries.append(code.replace("IMPORTDIR", import_dir)
                                                   .replace("RESOURCE", resource.lower()))
            
            # Special cases:
            elif i == "jensenlab":
                code = cypher_queries['IMPORT_JENSENLAB_DATA']['query']
                for (entity1, entity2) in config["jensenlabEntities"]:
                    queries.append(code.replace("IMPORTDIR", import_dir)
                                       .replace("ENTITY1", entity1)
                                       .replace("ENTITY2", entity2))
            
            elif i == "mentions":
                queries.append(cypher_queries['CREATE_PUBLICATIONS']['query'].replace("IMPORTDIR", import_dir))
                code = cypher_queries['IMPORT_MENTIONS']['query']
                for entity in config["mentionEntities"]:
                    queries.append(code.replace("IMPORTDIR", import_dir).replace("ENTITY", entity))
            
            elif i == "published":
                code = cypher_queries['IMPORT_PUBLISHED_IN']['query']
                for entity in config["publicationEntities"]:
                    queries.append(code.replace("IMPORTDIR", import_dir).replace("ENTITY", entity))
            
            elif i == "user":
                usersDir = ckg_config['imports_users_directory']
                queries.append(cypher_queries['CREATE_USER_NODE']['query'].replace("IMPORTDIR", usersDir))
            
            elif i == "project":
                import_dir = quote(ckg_config['imports_experiments_directory'], safe='/:')
                projects = builder_utils.listDirectoryFolders(import_dir)
                if len(specific) > 0:
                    projects = list(set(projects).intersection(specific))
                project_cypher = cypher_queries['IMPORT_PROJECT']
                for project in projects:
                    projectDir = os.path.join(import_dir, project, 'project').replace('\\','/')
                    for project_section in project_cypher:
                        queries.append(project_section['query'].replace("IMPORTDIR", projectDir)
                                                               .replace('PROJECTID', project))
            
            elif i == "experiment":
                import_dir = quote(ckg_config['imports_experiments_directory'], safe='/:')
                datasets_cypher = cypher_queries['IMPORT_DATASETS']
                projects = builder_utils.listDirectoryFolders(import_dir)
                if len(specific) > 0:
                    projects = list(set(projects).intersection(specific))
                for project in projects:
                    projectDir = os.path.join(import_dir, project).replace('\\', '/')
                    datasetTypes = builder_utils.listDirectoryFolders(projectDir)
                    for dtype in datasetTypes:
                        if dtype in datasets_cypher:
                            datasetDir = os.path.join(projectDir, dtype).replace('\\', '/')
                            queries.append(datasets_cypher[dtype]['query'].replace("IMPORTDIR", datasetDir)
                                                                          .replace('PROJECTID', project))
            
            else:
                logger.error(f"Non-existing dataset. The dataset you are trying to load does not exist: {i}.")
            
            # Execute queries
            load_into_database(driver, queries, i)
            print(f'Done Loading {i}')
        
        except Exception as err:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            logger.error(f"Loading: {i}: {err}, file: {fname}, line: {exc_tb.tb_lineno}")


def fullUpdate():
    """
    Main method that controls the population of the graph database. Firstly, it gets a connection \
    to the database (driver) and then initiates the update of the entire database getting \
    all the graph entities to update from configuration. Once the graph database has been \
    populated, the imports folder in data/ is compressed and archived in the archive/ folder \
    so that a backup of the imports files is kept (full).
    """
    imports = config["graph"]
    driver = connector.getGraphDatabaseConnectionConfiguration()
    logger.info("Full update of the database - Updating: {}".format(",".join(imports)))
    updateDB(driver, imports)
    logger.info("Full update of the database - Update took: {}".format(datetime.now() - START_TIME))
    logger.info("Full update of the database - Archiving imports folder")
    archiveImportDirectory(archive_type="full")
    logger.info("Full update of the database - Archiving took: {}".format(datetime.now() - START_TIME))


def partialUpdate(imports, specific=[]):
    """
    Method that controls the update of the graph database with the specified entities and \
    relationships. Firstly, it gets a connection to the database (driver) and then initiates \
    the update of the specified graph entities. \
    Once the graph database has been populated, the data files uploaded to the graph are compressed \
    and archived in the archive/ folder (partial).

    :param list imports: list of entities to update
    """
    driver = connector.getGraphDatabaseConnectionConfiguration()
    logger.info("Partial update of the database - Updating: {}".format(",".join(imports)))
    updateDB(driver, imports, specific)
    logger.info("Partial update of the database - Update took: {}".format(datetime.now() - START_TIME))
    logger.info("Partial update of the database - Archiving imports folder")
    #archiveImportDirectory(archive_type="partial")
    logger.info("Partial update of the database - Archiving {} took: {}".format(",".join(imports), datetime.now() - START_TIME))


def archiveImportDirectory(archive_type="full"):
    """
    This function creates the compressed backup imports folder with either the whole folder \
    (full update) or with only the files uploaded (partial update). The folder or files are \
    compressed into a gzipped tarball file and stored in the archive/ folder defined in the \
    configuration.

    :param str archive_type: whether it is a full update or a partial update.
    """
    dest_folder = ckg_config["archive_directory"]
    builder_utils.checkDirectory(dest_folder)
    folder_to_backup = ckg_config["imports_directory"]
    date, time = builder_utils.getCurrentTime()
    file_name = "{}_{}_{}".format(archive_type, date.replace('-', ''), time.replace(':', ''))
    logger.info("Archiving {} to file: {}".format(folder_to_backup, file_name))
    builder_utils.compress_directory(folder_to_backup, dest_folder, file_name)
    logger.info("New backup created: {}".format(file_name))

if __name__ == "__main__":
    pass